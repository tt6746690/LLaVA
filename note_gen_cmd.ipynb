{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc14e2cf-7b86-4071-b501-2ac1f3b63e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fsx/wpq/miniconda3/envs/llava/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import itertools\n",
    "import json\n",
    "import glob\n",
    "os.chdir('/fsx/wpq/github/metasummer2024/external/LLaVA') # jupyter lab moving ipynb does not change !pwd properly.\n",
    "import pandas as pd\n",
    "from rosemary.submit import shell_scripts_template, submit_job_slurm, multiline_to_singleline\n",
    "\n",
    "log_dir = '/fsx/wpq/.slurm_log'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b744a6d-dce1-48e8-92d4-b477d950802f",
   "metadata": {},
   "source": [
    "## Pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "18cf9bdf-f6a9-4b1a-8bb0-6c814633c208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"args\": \"sbatch /fsx/wpq/.sbatch/2024-06-07_05:42:49_07b8b584-5dcb-42fa-a8e7-ca92e451dcca.sh\",\n",
      "        \"job_id\": 350166\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "nodes = 1; num_gpus = 8; cpu_mem = 1000; num_cpus = 96\n",
    "report_to = 'wandb'\n",
    "data_path = './data/liuhaotian/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json'\n",
    "image_folder = './data/liuhaotian/LLaVA-Pretrain/images'\n",
    "train_size = None\n",
    "\n",
    "\n",
    "# ablate lm/vision/projector\n",
    "job_name = 'pt_1'\n",
    "model_name_or_path_list = [\n",
    "    './results/baselines/lmsys/vicuna-7b-v1.5',\n",
    "    # './results/baselines/NousResearch/Llama-2-7b-hf',\n",
    "    # './results/baselines/unsloth/llama-3-8b',\n",
    "]\n",
    "vision_tower_list = [\n",
    "    # './results/baselines/openai/clip-vit-large-patch14', # 224\n",
    "    './results/baselines/openai/clip-vit-large-patch14-336',\n",
    "]\n",
    "mm_projector_type_list = [\n",
    "    # 'linear',\n",
    "    'mlp2x_gelu',\n",
    "]\n",
    "\n",
    "\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "\n",
    "\n",
    "options_list = itertools.product(\n",
    "    model_name_or_path_list,\n",
    "    vision_tower_list,\n",
    "    mm_projector_type_list,\n",
    ")\n",
    "cmds = []\n",
    "\n",
    "for (\n",
    "    model_name_or_path,\n",
    "    vision_tower,\n",
    "    mm_projector_type,\n",
    ") in options_list:\n",
    "    output_dir  = f\"lm={os.path.basename(model_name_or_path)}\"\n",
    "    output_dir += f\"_vis={os.path.basename(vision_tower)}\"\n",
    "    output_dir += f\"_mm={mm_projector_type}\"\n",
    "    \n",
    "    output_dir = os.path.join('./results', job_name, output_dir)\n",
    "\n",
    "    os.environ['WANDB_NAME'] = output_dir.replace('./results/', '')\n",
    "    cmd = f\"\"\"\n",
    "    deepspeed llava/train/train_mem.py \\\n",
    "        --deepspeed ./scripts/zero2.json \\\n",
    "        --model_name_or_path {model_name_or_path} \\\n",
    "        --version plain \\\n",
    "        --data_path {data_path} \\\n",
    "        --image_folder {image_folder} \\\n",
    "        --vision_tower {vision_tower} \\\n",
    "        --mm_projector_type {mm_projector_type} \\\n",
    "        --tune_mm_mlp_adapter True \\\n",
    "        --mm_vision_select_layer -2 \\\n",
    "        --mm_use_im_start_end False \\\n",
    "        --mm_use_im_patch_token False \\\n",
    "        --bf16 True \\\n",
    "        {\"--train_size \" + str(train_size) if train_size else \"\"} \\\n",
    "        --num_train_epochs 1 \\\n",
    "        --per_device_train_batch_size 32 \\\n",
    "        --per_device_eval_batch_size 4 \\\n",
    "        --gradient_accumulation_steps 1 \\\n",
    "        --evaluation_strategy \"no\" \\\n",
    "        --save_strategy \"steps\" \\\n",
    "        --save_steps 24000 \\\n",
    "        --save_total_limit 1 \\\n",
    "        --learning_rate 1e-3 \\\n",
    "        --weight_decay 0. \\\n",
    "        --warmup_ratio 0.03 \\\n",
    "        --lr_scheduler_type \"cosine\" \\\n",
    "        --logging_steps 1 \\\n",
    "        --tf32 True \\\n",
    "        --model_max_length 2048 \\\n",
    "        --gradient_checkpointing True \\\n",
    "        --dataloader_num_workers 4 \\\n",
    "        --lazy_preprocess True \\\n",
    "        --report_to {'none' if test_run else report_to} \\\n",
    "        --output_dir {output_dir}\n",
    "    \"\"\"\n",
    "    \n",
    "    if test_run:\n",
    "        print('\\n'+' \\\\\\n\\t'.join([x.strip() for x in re.split(r'\\s{3,}', cmd)]))\n",
    "    \n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "    cmds.append(cmd)\n",
    "    \n",
    "    \n",
    "    shell_scripts = shell_scripts_template.format(\n",
    "        profile='/fsx/wpq/.profile_local.sh',\n",
    "        conda_env='llava',\n",
    "        cwd=os.getcwd(),\n",
    "        cmd=cmd,\n",
    "        log_dir=log_dir,\n",
    "        save_dir=output_dir\n",
    "    )\n",
    "    \n",
    "    out = submit_job_slurm(\n",
    "        shell_scripts,\n",
    "        job_name=job_name,\n",
    "        partition='learnai4p',\n",
    "        nodes=nodes,\n",
    "        num_cpus=num_cpus,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=num_gpus,\n",
    "        log_path=os.path.join(log_dir, '%J.out'),\n",
    "        test_run=test_run,\n",
    "    )\n",
    "    \n",
    "    print(json.dumps(out, indent=4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3de604b-d5c5-4c98-a59e-47e92d5af478",
   "metadata": {},
   "source": [
    "## Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6bf710-f220-4f1c-8a9c-ce31cebecdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = 1; num_gpus = 8; cpu_mem = 1000; num_cpus = 96\n",
    "report_to = 'wandb'\n",
    "train_size = None # all data\n",
    "data_path = './data/liuhaotian/LLaVA-Instruct-150K/llava_v1_5_mix665k.json'\n",
    "image_folder = './data/'\n",
    "\n",
    "# model_name_or_path = './results/baselines/lmsys/vicuna-7b-v1.5'\n",
    "# pretrain_mm_mlp_adapter = './results/pretrain/llava-v1.5-7b/mm_projector.bin'\n",
    "# vision_tower = './results/baselines/openai/clip-vit-large-patch14-336'\n",
    "# mm_projector_type = 'mlp2x_gelu'\n",
    "\n",
    "# ablate lm/vision/projector\n",
    "job_name = 'sft_1'\n",
    "model_name_or_path_list = [\n",
    "    './results/baselines/lmsys/vicuna-7b-v1.5',\n",
    "    './results/baselines/NousResearch/Llama-2-7b-hf',\n",
    "]\n",
    "vision_tower_list = [\n",
    "    './results/baselines/openai/clip-vit-large-patch14', # 224\n",
    "    # './results/baselines/openai/clip-vit-large-patch14-336',\n",
    "]\n",
    "mm_projector_type_list = [\n",
    "    # 'linear',\n",
    "    'mlp2x_gelu',\n",
    "]\n",
    "\n",
    "\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "\n",
    "\n",
    "options_list = itertools.product(\n",
    "    model_name_or_path_list,\n",
    "    vision_tower_list,\n",
    "    mm_projector_type_list,\n",
    ")\n",
    "\n",
    "cmds = []\n",
    "\n",
    "\n",
    "for (\n",
    "    model_name_or_path,\n",
    "    vision_tower,\n",
    "    mm_projector_type,\n",
    ") in options_list:\n",
    "    output_dir  = f\"lm={os.path.basename(model_name_or_path)}\"\n",
    "    output_dir += f\"_vis={os.path.basename(vision_tower)}\"\n",
    "    output_dir += f\"_mm={mm_projector_type}\"\n",
    "    \n",
    "    output_dir = os.path.join('./results', job_name, output_dir)\n",
    "\n",
    "    mm_adaptor_job_dir = output_dir.replace(job_name, job_name.replace('sft', 'pt'))\n",
    "    pretrain_mm_mlp_adapter = os.path.join(mm_adaptor_job_dir, 'mm_projector.bin')\n",
    "    if not os.path.isfile(pretrain_mm_mlp_adapter):\n",
    "        raise ValueError(f'{pretrain_mm_mlp_adapter} does not exists.')\n",
    "\n",
    "    os.environ['WANDB_NAME'] = output_dir.replace('./results/', '')\n",
    "\n",
    "    cmd = f\"\"\"\n",
    "    deepspeed llava/train/train_mem.py \\\n",
    "        --deepspeed ./scripts/zero3.json \\\n",
    "        --model_name_or_path {model_name_or_path} \\\n",
    "        --version v1 \\\n",
    "        --data_path {data_path} \\\n",
    "        --image_folder {image_folder} \\\n",
    "        --vision_tower {vision_tower} \\\n",
    "        --pretrain_mm_mlp_adapter {pretrain_mm_mlp_adapter} \\\n",
    "        --mm_projector_type {mm_projector_type} \\\n",
    "        --mm_vision_select_layer -2 \\\n",
    "        --mm_use_im_start_end False \\\n",
    "        --mm_use_im_patch_token False \\\n",
    "        --image_aspect_ratio pad \\\n",
    "        --group_by_modality_length True \\\n",
    "        --bf16 True \\\n",
    "        {\"--train_size \" + str(train_size) if train_size else \"\"} \\\n",
    "        --num_train_epochs 1 \\\n",
    "        --per_device_train_batch_size 16 \\\n",
    "        --per_device_eval_batch_size 4 \\\n",
    "        --gradient_accumulation_steps 1 \\\n",
    "        --evaluation_strategy \"no\" \\\n",
    "        --save_strategy \"steps\" \\\n",
    "        --save_steps 50000 \\\n",
    "        --save_total_limit 1 \\\n",
    "        --learning_rate 2e-5 \\\n",
    "        --weight_decay 0. \\\n",
    "        --warmup_ratio 0.03 \\\n",
    "        --lr_scheduler_type \"cosine\" \\\n",
    "        --logging_steps 1 \\\n",
    "        --tf32 True \\\n",
    "        --model_max_length 2048 \\\n",
    "        --gradient_checkpointing True \\\n",
    "        --dataloader_num_workers 4 \\\n",
    "        --lazy_preprocess True \\\n",
    "        --report_to {'none' if test_run else report_to} \\\n",
    "        --output_dir {output_dir}\n",
    "    \"\"\"\n",
    "    \n",
    "    if test_run:\n",
    "        print('\\n'+' \\\\\\n\\t'.join([x.strip() for x in re.split(r'\\s{3,}', cmd)]))\n",
    "    \n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "    cmds.append(cmd)\n",
    "    \n",
    "    \n",
    "    shell_scripts = shell_scripts_template.format(\n",
    "        profile='/fsx/wpq/.profile_local.sh',\n",
    "        conda_env='llava',\n",
    "        cwd=os.getcwd(),\n",
    "        cmd=cmd,\n",
    "        log_dir=log_dir,\n",
    "        save_dir=output_dir\n",
    "    )\n",
    "    \n",
    "    out = submit_job_slurm(\n",
    "        shell_scripts,\n",
    "        job_name=job_name,\n",
    "        partition='learnai4p',\n",
    "        nodes=nodes,\n",
    "        num_cpus=num_cpus,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=num_gpus,\n",
    "        log_path=os.path.join(log_dir, '%J.out'),\n",
    "        test_run=test_run,\n",
    "    )\n",
    "    \n",
    "    print(json.dumps(out, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1233839e-ca4d-4ec8-bf49-d2ccce11b9af",
   "metadata": {},
   "source": [
    "## Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32a4280e-30e3-4b62-a0ad-952a2d2fa839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloed https://evalai.s3.amazonaws.com/media/submission_files/submission_451709/7decbad8-5ee2-4c10-93ff-aad4ad0d7a5c.json\n",
      "\t->results/baselines/liuhaotian/llava-v1.5-7b/eval/vizwiz\n",
      "Downloed https://evalai.s3.amazonaws.com/media/submission_files/submission_450628/9bef3c4f-d7e2-4ff9-8d8a-84768881b20a.json\n",
      "\t->results/baselines/liuhaotian/llava-v1.5-7b/eval/vqav2\n",
      "Downloed https://evalai.s3.amazonaws.com/media/submission_files/submission_452449/1abc8d56-b08f-44ca-a6a8-88edb742d940.json\n",
      "\t->results/sft_1/lm=Llama-2-7b-hf_vis=clip-vit-large-patch14-336_mm=mlp2x_gelu/eval/vizwiz\n",
      "Downloed https://evalai.s3.amazonaws.com/media/submission_files/submission_452451/08618a88-3928-47f4-a352-673ac10dc1c5.json\n",
      "\t->results/sft_1/lm=Llama-2-7b-hf_vis=clip-vit-large-patch14-336_mm=mlp2x_gelu/eval/vqav2\n"
     ]
    }
   ],
   "source": [
    "from utils import download_eval_server_results\n",
    "download_eval_server_results('./eval_server_results.csv', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "37d5f326-23b8-4754-9069-ceadc8360cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmds: #= 1\n",
      "[('llavabench', 'results/baselines/liuhaotian/llava-v1.5-7b')] \n",
      "\n",
      "\n",
      "{\n",
      "    \"args\": \"sbatch /fsx/wpq/.sbatch/2024-06-14_00:42:06_b517823a-482c-4d57-9ed3-b2eca9be6c26.sh\",\n",
      "    \"cmd\": \"bash scripts/v1_5/eval_mod/llavabench.sh results/baselines/liuhaotian/llava-v1.5-7b > results/baselines/liuhaotian/llava-v1.5-7b/eval/llavabench/bash_script_log.txt\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from rosemary.submit import submit_job_slurm\n",
    "nodes = 1; num_gpus = 8; cpu_mem = 1000; num_cpus = 96\n",
    "\n",
    "eval_script_dir = 'scripts/v1_5/eval_mod'\n",
    "eval_rest = True\n",
    "subdir_path_list = []\n",
    "subdir_filter_fn = lambda x: True\n",
    "\n",
    "\n",
    "task_names_vqa = ['vqav2', 'gqa', 'vizwiz', 'textvqa', 'scienceqa']\n",
    "task_names_sft = ['pope', 'mme', 'mmbench', 'seed', 'llavabench', 'mmvet']\n",
    "task_names_all = task_names_vqa + task_names_sft\n",
    "task_names_llm_evaluator = ['llavabench', 'mmvet']\n",
    "\n",
    "######\n",
    "\n",
    "## baselines\n",
    "subdir_path_list = [\n",
    "    'results/baselines/liuhaotian/llava-v1.5-7b',\n",
    "]\n",
    "task_names = ['llavabench']\n",
    "eval_rest = False\n",
    "\n",
    "# # sft_1\n",
    "# exp_dirs = ['results/sft_1']\n",
    "# subdir_filter_fn = lambda x: 'Llama-2' in x\n",
    "# task_names = ['pope', 'mme', 'mmbench', 'seed']\n",
    "# eval_rest = False\n",
    "\n",
    "\n",
    "######\n",
    "\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "\n",
    "if len(subdir_path_list)==0:\n",
    "    subdir_path_list = []\n",
    "    for exp_dir in exp_dirs:\n",
    "        subdirs = list(os.listdir(exp_dir))\n",
    "        subdirs = filter(subdir_filter_fn, subdirs)\n",
    "        for subdir in subdirs:\n",
    "            subdir_path = os.path.join(exp_dir, subdir)\n",
    "            if not os.path.isfile(os.path.join(subdir_path, 'config.json')): # skip runs not yet finished\n",
    "                continue\n",
    "            subdir_path_list.append(subdir_path)\n",
    "\n",
    "@dataclass\n",
    "class TaskConfig:\n",
    "    task_finish_proof: str\n",
    "    num_gpus: int\n",
    "    bash_script_name: str\n",
    "\n",
    "task_configs = {\n",
    "    'vqav2': TaskConfig('*/answers_upload.json', 8, 'vqav2'),\n",
    "    'gqa': TaskConfig('*/*_predictions.json', 8, 'gqa'),\n",
    "    'vizwiz': TaskConfig('answers_upload.json', 1, 'vizwiz'),\n",
    "    'textvqa': TaskConfig('answers.jsonl', 1, 'textvqa'),\n",
    "    'scienceqa': TaskConfig('results.jsonl', 1, 'sqa'),\n",
    "    'pope': TaskConfig('answers.jsonl', 1, 'pope'),\n",
    "    'mme': TaskConfig('results', 1, 'mme'),\n",
    "    'mmbench': TaskConfig('*.xlsx', 1, 'mmbench'),\n",
    "    'mmvet': TaskConfig('results_*.csv', 1, 'mmvet'),\n",
    "    'seed': TaskConfig('answers_upload.jsonl', 8, 'seed'),\n",
    "    'llavabench': TaskConfig('reviews.jsonl', 1, 'llavabench'),\n",
    "}\n",
    "\n",
    "if eval_rest:\n",
    "    task_name_and_model = []\n",
    "    for subdir_path in subdir_path_list:\n",
    "        for task_name in task_names:\n",
    "            if not glob.glob(os.path.join(subdir_path, 'eval', task_name, task_configs[task_name].task_finish_proof)):\n",
    "                task_name_and_model.append((task_name, subdir_path))\n",
    "else:\n",
    "    task_name_and_model = list(itertools.product(task_names, subdir_path_list))\n",
    "    \n",
    "print('cmds: #=', len(list(task_name_and_model)))\n",
    "print(list(task_name_and_model), '\\n\\n')\n",
    "\n",
    "\n",
    "dfo = pd.DataFrame(task_name_and_model, columns=['task_name', 'model_name_or_path'])\n",
    "model_and_task_list = dfo.groupby('model_name_or_path')['task_name'].agg(list).to_dict()\n",
    "\n",
    "cmds = []\n",
    "for model_name_or_path, task_name_list in model_and_task_list.items():\n",
    "    model_name_or_path = model_name_or_path.rstrip('/')\n",
    "    for i, task_name in enumerate(task_name_list):\n",
    "        task_config = task_configs[task_name]\n",
    "        job_name = f'eval.{task_name}'\n",
    "        save_dir = f'{model_name_or_path}/eval/{task_name}'\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        if task_name.startswith(tuple(task_configs.keys())):\n",
    "            cmd = f\"bash {eval_script_dir}/{task_config.bash_script_name}.sh {model_name_or_path} > {os.path.join(save_dir,'bash_script_log.txt')}\"\n",
    "        else:\n",
    "            raise ValueError(f'{task_name} not supported.')\n",
    "\n",
    "        num_gpus = task_config.num_gpus            \n",
    "        cmd = multiline_to_singleline(cmd)\n",
    "        cmds.append(cmd)\n",
    "        \n",
    "        shell_scripts = shell_scripts_template.format(\n",
    "            profile='/fsx/wpq/.profile_local.sh',\n",
    "            conda_env='llava',\n",
    "            cwd=os.getcwd(),\n",
    "            cmd=cmd,\n",
    "            log_dir=log_dir,\n",
    "            save_dir=save_dir\n",
    "        )\n",
    "        out = submit_job_slurm(\n",
    "            shell_scripts, \n",
    "            job_name=job_name,\n",
    "            partition='learnai4p',\n",
    "            nodes=nodes,\n",
    "            num_cpus=num_cpus,\n",
    "            cpu_mem=cpu_mem,\n",
    "            num_gpus=num_gpus,\n",
    "        log_path=os.path.join(log_dir, '%J.out'),\n",
    "            test_run=test_run,\n",
    "        )\n",
    "        for x in out:\n",
    "            x.update({'cmd': cmd})\n",
    "\n",
    "        print(json.dumps(out[0], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff0e0a43-fec6-44eb-85b5-3edf31949182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'perception': {'total_score': 1506.7612044817927,\n",
       "  'existence_score': 190.0,\n",
       "  'count_score': 155.0,\n",
       "  'position_score': 128.33333333333334,\n",
       "  'color_score': 170.0,\n",
       "  'posters_score': 147.61904761904762,\n",
       "  'celebrity_score': 137.05882352941177,\n",
       "  'scene_score': 158.0,\n",
       "  'landmark_score': 163.75,\n",
       "  'artwork_score': 119.5,\n",
       "  'ocr_score': 137.5},\n",
       " 'cognition': {'total_score': 355.7142857142857,\n",
       "  'commonsense_reasoning_score': 110.71428571428571,\n",
       "  'numerical_calculation_score': 70.0,\n",
       "  'text_translation_score': 107.5,\n",
       "  'code_reasoning_score': 67.5}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import TaskResult\n",
    "    \n",
    "        \n",
    "\n",
    "task_name = 'mme'\n",
    "# save_dir = f'/fsx/wpq/github/metasummer2024/external/LLaVA/results/sft_1/lm=Llama-2-7b-hf_vis=clip-vit-large-patch14-336_mm=mlp2x_gelu/eval/{task_name}'\n",
    "save_dir = f'/fsx/wpq/github/metasummer2024/external/LLaVA/results/baselines/liuhaotian/llava-v1.5-7b/eval/{task_name}'\n",
    "\n",
    "r = TaskResult(save_dir)\n",
    "r.get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "b5c500d1-5cb2-40f2-a8da-d475bbd381bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'reviews\\nall 65.3 90.8 59.3\\nllava_bench_complex 79.3 87.1 69.1\\nllava_bench_conv 51.5 96.5 49.7\\nllava_bench_detail 56.9 91.3 52.0\\n=================================\\n'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "b711e501-94e5-4538-afa2-ff022dcb224e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all': {'score_model/score_ref': 65.3,\n",
       "  'score_ref': 90.8,\n",
       "  'score_model': 59.3},\n",
       " 'llava_bench_complex': {'score_model/score_ref': 79.3,\n",
       "  'score_ref': 87.1,\n",
       "  'score_model': 69.1},\n",
       " 'llava_bench_conv': {'score_model/score_ref': 51.5,\n",
       "  'score_ref': 96.5,\n",
       "  'score_model': 49.7},\n",
       " 'llava_bench_detail': {'score_model/score_ref': 56.9,\n",
       "  'score_ref': 91.3,\n",
       "  'score_model': 52.0}}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd43703-532d-44ef-802e-dd60c369f86f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:llava]",
   "language": "python",
   "name": "conda-env-llava-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
