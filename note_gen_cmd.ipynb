{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc14e2cf-7b86-4071-b501-2ac1f3b63e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import itertools\n",
    "import json\n",
    "import glob\n",
    "os.chdir('/fsx/wpq/github/metasummer2024/external/LLaVA') # jupyter lab moving ipynb does not change !pwd properly.\n",
    "import pandas as pd\n",
    "from rosemary.submit import shell_scripts_template, submit_job_slurm, multiline_to_singleline\n",
    "\n",
    "log_dir = '/fsx/wpq/.slurm_log'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b744a6d-dce1-48e8-92d4-b477d950802f",
   "metadata": {},
   "source": [
    "## Pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "18cf9bdf-f6a9-4b1a-8bb0-6c814633c208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"args\": \"sbatch /fsx/wpq/.sbatch/2024-06-07_05:42:49_07b8b584-5dcb-42fa-a8e7-ca92e451dcca.sh\",\n",
      "        \"job_id\": 350166\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "nodes = 1; num_gpus = 8; cpu_mem = 1000; num_cpus = 96\n",
    "report_to = 'wandb'\n",
    "data_path = './data/liuhaotian/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json'\n",
    "image_folder = './data/liuhaotian/LLaVA-Pretrain/images'\n",
    "train_size = None\n",
    "\n",
    "\n",
    "# ablate lm/vision/projector\n",
    "job_name = 'pt_1'\n",
    "model_name_or_path_list = [\n",
    "    './results/baselines/lmsys/vicuna-7b-v1.5',\n",
    "    # './results/baselines/NousResearch/Llama-2-7b-hf',\n",
    "    # './results/baselines/unsloth/llama-3-8b',\n",
    "]\n",
    "vision_tower_list = [\n",
    "    # './results/baselines/openai/clip-vit-large-patch14', # 224\n",
    "    './results/baselines/openai/clip-vit-large-patch14-336',\n",
    "]\n",
    "mm_projector_type_list = [\n",
    "    # 'linear',\n",
    "    'mlp2x_gelu',\n",
    "]\n",
    "\n",
    "\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "\n",
    "\n",
    "options_list = itertools.product(\n",
    "    model_name_or_path_list,\n",
    "    vision_tower_list,\n",
    "    mm_projector_type_list,\n",
    ")\n",
    "cmds = []\n",
    "\n",
    "for (\n",
    "    model_name_or_path,\n",
    "    vision_tower,\n",
    "    mm_projector_type,\n",
    ") in options_list:\n",
    "    output_dir  = f\"lm={os.path.basename(model_name_or_path)}\"\n",
    "    output_dir += f\"_vis={os.path.basename(vision_tower)}\"\n",
    "    output_dir += f\"_mm={mm_projector_type}\"\n",
    "    \n",
    "    output_dir = os.path.join('./results', job_name, output_dir)\n",
    "\n",
    "    os.environ['WANDB_NAME'] = output_dir.replace('./results/', '')\n",
    "    cmd = f\"\"\"\n",
    "    deepspeed llava/train/train_mem.py \\\n",
    "        --deepspeed ./scripts/zero2.json \\\n",
    "        --model_name_or_path {model_name_or_path} \\\n",
    "        --version plain \\\n",
    "        --data_path {data_path} \\\n",
    "        --image_folder {image_folder} \\\n",
    "        --vision_tower {vision_tower} \\\n",
    "        --mm_projector_type {mm_projector_type} \\\n",
    "        --tune_mm_mlp_adapter True \\\n",
    "        --mm_vision_select_layer -2 \\\n",
    "        --mm_use_im_start_end False \\\n",
    "        --mm_use_im_patch_token False \\\n",
    "        --bf16 True \\\n",
    "        {\"--train_size \" + str(train_size) if train_size else \"\"} \\\n",
    "        --num_train_epochs 1 \\\n",
    "        --per_device_train_batch_size 32 \\\n",
    "        --per_device_eval_batch_size 4 \\\n",
    "        --gradient_accumulation_steps 1 \\\n",
    "        --evaluation_strategy \"no\" \\\n",
    "        --save_strategy \"steps\" \\\n",
    "        --save_steps 24000 \\\n",
    "        --save_total_limit 1 \\\n",
    "        --learning_rate 1e-3 \\\n",
    "        --weight_decay 0. \\\n",
    "        --warmup_ratio 0.03 \\\n",
    "        --lr_scheduler_type \"cosine\" \\\n",
    "        --logging_steps 1 \\\n",
    "        --tf32 True \\\n",
    "        --model_max_length 2048 \\\n",
    "        --gradient_checkpointing True \\\n",
    "        --dataloader_num_workers 4 \\\n",
    "        --lazy_preprocess True \\\n",
    "        --report_to {'none' if test_run else report_to} \\\n",
    "        --output_dir {output_dir}\n",
    "    \"\"\"\n",
    "    \n",
    "    if test_run:\n",
    "        print('\\n'+' \\\\\\n\\t'.join([x.strip() for x in re.split(r'\\s{3,}', cmd)]))\n",
    "    \n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "    cmds.append(cmd)\n",
    "    \n",
    "    \n",
    "    shell_scripts = shell_scripts_template.format(\n",
    "        profile='/fsx/wpq/.profile_local.sh',\n",
    "        conda_env='llava',\n",
    "        cwd=os.getcwd(),\n",
    "        cmd=cmd,\n",
    "        log_dir=log_dir,\n",
    "        save_dir=output_dir\n",
    "    )\n",
    "    \n",
    "    out = submit_job_slurm(\n",
    "        shell_scripts,\n",
    "        job_name=job_name,\n",
    "        partition='learnai4p',\n",
    "        nodes=nodes,\n",
    "        num_cpus=num_cpus,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=num_gpus,\n",
    "        log_path=os.path.join(log_dir, '%J.out'),\n",
    "        test_run=test_run,\n",
    "    )\n",
    "    \n",
    "    print(json.dumps(out, indent=4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3de604b-d5c5-4c98-a59e-47e92d5af478",
   "metadata": {},
   "source": [
    "## Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6bf710-f220-4f1c-8a9c-ce31cebecdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = 1; num_gpus = 8; cpu_mem = 1000; num_cpus = 96\n",
    "report_to = 'wandb'\n",
    "train_size = None # all data\n",
    "data_path = './data/liuhaotian/LLaVA-Instruct-150K/llava_v1_5_mix665k.json'\n",
    "image_folder = './data/'\n",
    "\n",
    "# model_name_or_path = './results/baselines/lmsys/vicuna-7b-v1.5'\n",
    "# pretrain_mm_mlp_adapter = './results/pretrain/llava-v1.5-7b/mm_projector.bin'\n",
    "# vision_tower = './results/baselines/openai/clip-vit-large-patch14-336'\n",
    "# mm_projector_type = 'mlp2x_gelu'\n",
    "\n",
    "# ablate lm/vision/projector\n",
    "job_name = 'sft_1'\n",
    "model_name_or_path_list = [\n",
    "    './results/baselines/lmsys/vicuna-7b-v1.5',\n",
    "    './results/baselines/NousResearch/Llama-2-7b-hf',\n",
    "]\n",
    "vision_tower_list = [\n",
    "    './results/baselines/openai/clip-vit-large-patch14', # 224\n",
    "    # './results/baselines/openai/clip-vit-large-patch14-336',\n",
    "]\n",
    "mm_projector_type_list = [\n",
    "    # 'linear',\n",
    "    'mlp2x_gelu',\n",
    "]\n",
    "\n",
    "\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "\n",
    "\n",
    "options_list = itertools.product(\n",
    "    model_name_or_path_list,\n",
    "    vision_tower_list,\n",
    "    mm_projector_type_list,\n",
    ")\n",
    "\n",
    "cmds = []\n",
    "\n",
    "\n",
    "for (\n",
    "    model_name_or_path,\n",
    "    vision_tower,\n",
    "    mm_projector_type,\n",
    ") in options_list:\n",
    "    output_dir  = f\"lm={os.path.basename(model_name_or_path)}\"\n",
    "    output_dir += f\"_vis={os.path.basename(vision_tower)}\"\n",
    "    output_dir += f\"_mm={mm_projector_type}\"\n",
    "    \n",
    "    output_dir = os.path.join('./results', job_name, output_dir)\n",
    "\n",
    "    mm_adaptor_job_dir = output_dir.replace(job_name, job_name.replace('sft', 'pt'))\n",
    "    pretrain_mm_mlp_adapter = os.path.join(mm_adaptor_job_dir, 'mm_projector.bin')\n",
    "    if not os.path.isfile(pretrain_mm_mlp_adapter):\n",
    "        raise ValueError(f'{pretrain_mm_mlp_adapter} does not exists.')\n",
    "\n",
    "    os.environ['WANDB_NAME'] = output_dir.replace('./results/', '')\n",
    "\n",
    "    cmd = f\"\"\"\n",
    "    deepspeed llava/train/train_mem.py \\\n",
    "        --deepspeed ./scripts/zero3.json \\\n",
    "        --model_name_or_path {model_name_or_path} \\\n",
    "        --version v1 \\\n",
    "        --data_path {data_path} \\\n",
    "        --image_folder {image_folder} \\\n",
    "        --vision_tower {vision_tower} \\\n",
    "        --pretrain_mm_mlp_adapter {pretrain_mm_mlp_adapter} \\\n",
    "        --mm_projector_type {mm_projector_type} \\\n",
    "        --mm_vision_select_layer -2 \\\n",
    "        --mm_use_im_start_end False \\\n",
    "        --mm_use_im_patch_token False \\\n",
    "        --image_aspect_ratio pad \\\n",
    "        --group_by_modality_length True \\\n",
    "        --bf16 True \\\n",
    "        {\"--train_size \" + str(train_size) if train_size else \"\"} \\\n",
    "        --num_train_epochs 1 \\\n",
    "        --per_device_train_batch_size 16 \\\n",
    "        --per_device_eval_batch_size 4 \\\n",
    "        --gradient_accumulation_steps 1 \\\n",
    "        --evaluation_strategy \"no\" \\\n",
    "        --save_strategy \"steps\" \\\n",
    "        --save_steps 50000 \\\n",
    "        --save_total_limit 1 \\\n",
    "        --learning_rate 2e-5 \\\n",
    "        --weight_decay 0. \\\n",
    "        --warmup_ratio 0.03 \\\n",
    "        --lr_scheduler_type \"cosine\" \\\n",
    "        --logging_steps 1 \\\n",
    "        --tf32 True \\\n",
    "        --model_max_length 2048 \\\n",
    "        --gradient_checkpointing True \\\n",
    "        --dataloader_num_workers 4 \\\n",
    "        --lazy_preprocess True \\\n",
    "        --report_to {'none' if test_run else report_to} \\\n",
    "        --output_dir {output_dir}\n",
    "    \"\"\"\n",
    "    \n",
    "    if test_run:\n",
    "        print('\\n'+' \\\\\\n\\t'.join([x.strip() for x in re.split(r'\\s{3,}', cmd)]))\n",
    "    \n",
    "    cmd = multiline_to_singleline(cmd)\n",
    "    cmds.append(cmd)\n",
    "    \n",
    "    \n",
    "    shell_scripts = shell_scripts_template.format(\n",
    "        profile='/fsx/wpq/.profile_local.sh',\n",
    "        conda_env='llava',\n",
    "        cwd=os.getcwd(),\n",
    "        cmd=cmd,\n",
    "        log_dir=log_dir,\n",
    "        save_dir=output_dir\n",
    "    )\n",
    "    \n",
    "    out = submit_job_slurm(\n",
    "        shell_scripts,\n",
    "        job_name=job_name,\n",
    "        partition='learnai4p',\n",
    "        nodes=nodes,\n",
    "        num_cpus=num_cpus,\n",
    "        cpu_mem=cpu_mem,\n",
    "        num_gpus=num_gpus,\n",
    "        log_path=os.path.join(log_dir, '%J.out'),\n",
    "        test_run=test_run,\n",
    "    )\n",
    "    \n",
    "    print(json.dumps(out, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1233839e-ca4d-4ec8-bf49-d2ccce11b9af",
   "metadata": {},
   "source": [
    "## Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "37d5f326-23b8-4754-9069-ceadc8360cc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmds: #= 4\n",
      "[('vqav2', 'results/sft_1/lm=Llama-2-7b-hf_vis=clip-vit-large-patch14-336_mm=mlp2x_gelu'), ('gqa', 'results/sft_1/lm=Llama-2-7b-hf_vis=clip-vit-large-patch14-336_mm=mlp2x_gelu'), ('vizwiz', 'results/sft_1/lm=Llama-2-7b-hf_vis=clip-vit-large-patch14-336_mm=mlp2x_gelu'), ('textvqa', 'results/sft_1/lm=Llama-2-7b-hf_vis=clip-vit-large-patch14-336_mm=mlp2x_gelu')] \n",
      "\n",
      "\n",
      "{\n",
      "    \"args\": \"sbatch /fsx/wpq/.sbatch/2024-06-13_22:26:31_9b626cba-cf8a-4b7a-b9a4-c5f9ac51d19a.sh\",\n",
      "    \"job_id\": 350360,\n",
      "    \"cmd\": \"bash scripts/v1_5/eval_mod/vqav2.sh results/sft_1/lm=Llama-2-7b-hf_vis=clip-vit-large-patch14-336_mm=mlp2x_gelu > results/sft_1/lm=Llama-2-7b-hf_vis=clip-vit-large-patch14-336_mm=mlp2x_gelu/eval/vqav2/bash_script_log.txt\"\n",
      "}\n",
      "{\n",
      "    \"args\": \"sbatch /fsx/wpq/.sbatch/2024-06-13_22:26:31_e383177c-75f3-48d1-bc80-ecf9b10a657e.sh\",\n",
      "    \"job_id\": 350361,\n",
      "    \"cmd\": \"bash scripts/v1_5/eval_mod/gqa.sh results/sft_1/lm=Llama-2-7b-hf_vis=clip-vit-large-patch14-336_mm=mlp2x_gelu > results/sft_1/lm=Llama-2-7b-hf_vis=clip-vit-large-patch14-336_mm=mlp2x_gelu/eval/gqa/bash_script_log.txt\"\n",
      "}\n",
      "{\n",
      "    \"args\": \"sbatch /fsx/wpq/.sbatch/2024-06-13_22:26:31_39beecde-c89e-45fb-b492-ac2e1f2568bb.sh\",\n",
      "    \"job_id\": 350362,\n",
      "    \"cmd\": \"bash scripts/v1_5/eval_mod/vizwiz.sh results/sft_1/lm=Llama-2-7b-hf_vis=clip-vit-large-patch14-336_mm=mlp2x_gelu > results/sft_1/lm=Llama-2-7b-hf_vis=clip-vit-large-patch14-336_mm=mlp2x_gelu/eval/vizwiz/bash_script_log.txt\"\n",
      "}\n",
      "{\n",
      "    \"args\": \"sbatch /fsx/wpq/.sbatch/2024-06-13_22:26:31_2cb0bced-d186-4cc6-8530-50b67375b58a.sh\",\n",
      "    \"job_id\": 350363,\n",
      "    \"cmd\": \"bash scripts/v1_5/eval_mod/textvqa.sh results/sft_1/lm=Llama-2-7b-hf_vis=clip-vit-large-patch14-336_mm=mlp2x_gelu > results/sft_1/lm=Llama-2-7b-hf_vis=clip-vit-large-patch14-336_mm=mlp2x_gelu/eval/textvqa/bash_script_log.txt\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from rosemary.submit import submit_job_slurm\n",
    "nodes = 1; num_gpus = 8; cpu_mem = 1000; num_cpus = 96\n",
    "\n",
    "eval_script_dir = 'scripts/v1_5/eval_mod'\n",
    "eval_rest = True\n",
    "subdir_path_list = []\n",
    "subdir_filter_fn = lambda x: True\n",
    "\n",
    "task_names_all = [\n",
    "    # qa\n",
    "    'vqav2',\n",
    "    'gqa',\n",
    "    'vizwiz',\n",
    "    'textvqa',\n",
    "    'scienceqa',\n",
    "    # sft\n",
    "    'pope',\n",
    "    'mme',\n",
    "    'mmbench',\n",
    "    'mmvet',\n",
    "    'seed',\n",
    "    'llavabench',\n",
    "]\n",
    "\n",
    "\n",
    "task_names_vqa = ['vqav2', 'gqa', 'vizwiz', 'textvqa', 'scienceqa']\n",
    "\n",
    "\n",
    "######\n",
    "\n",
    "# ## baselines\n",
    "# subdir_path_list = [\n",
    "#     'results/baselines/liuhaotian/llava-v1.5-7b',\n",
    "# ]\n",
    "\n",
    "# sft_1\n",
    "exp_dirs = ['results/sft_1']\n",
    "subdir_filter_fn = lambda x: 'Llama-2' in x\n",
    "task_names = task_names_vqa\n",
    "task_names = task_names_vqa[:4]; eval_rest = False\n",
    "\n",
    "\n",
    "\n",
    "######\n",
    "\n",
    "test_run = 1\n",
    "test_run = bool(test_run)\n",
    "\n",
    "if len(subdir_path_list)==0:\n",
    "    subdir_path_list = []\n",
    "    for exp_dir in exp_dirs:\n",
    "        subdirs = list(os.listdir(exp_dir))\n",
    "        subdirs = filter(subdir_filter_fn, subdirs)\n",
    "        for subdir in subdirs:\n",
    "            subdir_path = os.path.join(exp_dir, subdir)\n",
    "            if not os.path.isfile(os.path.join(subdir_path, 'config.json')): # skip runs not yet finished\n",
    "                continue\n",
    "            subdir_path_list.append(subdir_path)\n",
    "\n",
    "@dataclass\n",
    "class TaskConfig:\n",
    "    task_finish_proof: str\n",
    "    num_gpus: int\n",
    "    bash_script_name: str\n",
    "\n",
    "task_configs = {\n",
    "    'vqav2': TaskConfig('*/answers_upload.json', 8, 'vqav2'),\n",
    "    'gqa': TaskConfig('*/*_predictions.json', 8, 'gqa'),\n",
    "    'vizwiz': TaskConfig('answers_upload.json', 1, 'vizwiz'),\n",
    "    'textvqa': TaskConfig('answers.jsonl', 1, 'textvqa'),\n",
    "    'scienceqa': TaskConfig('results.jsonl', 1, 'sqa'),\n",
    "    'pope': TaskConfig('answers.jsonl', 1, 'pope'),\n",
    "    'mme': TaskConfig('results', 1, 'mme'),\n",
    "    'mmbench': TaskConfig('*.xlsx', 1, 'mmbench'),\n",
    "    'mmvet': TaskConfig('results_*.csv', 1, 'mmvet'),\n",
    "    'seed': TaskConfig('answers_upload.jsonl', 8, 'seed'),\n",
    "    'llavabench': TaskConfig('reviews.jsonl', 1, 'llavabench'),\n",
    "}\n",
    "\n",
    "if eval_rest:\n",
    "    task_name_and_model = []\n",
    "    for subdir_path in subdir_path_list:\n",
    "        for task_name in task_names:\n",
    "            if not glob.glob(os.path.join(subdir_path, 'eval', task_name, task_configs[task_name].task_finish_proof)):\n",
    "                task_name_and_model.append((task_name, subdir_path))\n",
    "else:\n",
    "    task_name_and_model = list(itertools.product(task_names, subdir_path_list))\n",
    "    \n",
    "print('cmds: #=', len(list(task_name_and_model)))\n",
    "print(list(task_name_and_model), '\\n\\n')\n",
    "\n",
    "\n",
    "dfo = pd.DataFrame(task_name_and_model, columns=['task_name', 'model_name_or_path'])\n",
    "model_and_task_list = dfo.groupby('model_name_or_path')['task_name'].agg(list).to_dict()\n",
    "\n",
    "cmds = []\n",
    "for model_name_or_path, task_name_list in model_and_task_list.items():\n",
    "    model_name_or_path = model_name_or_path.rstrip('/')\n",
    "    for i, task_name in enumerate(task_name_list):\n",
    "        task_config = task_configs[task_name]\n",
    "        job_name = f'eval.{task_name}'\n",
    "        save_dir = f'{model_name_or_path}/eval/{task_name}'\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        if task_name.startswith(tuple(task_configs.keys())):\n",
    "            cmd = f\"bash {eval_script_dir}/{task_config.bash_script_name}.sh {model_name_or_path} > {os.path.join(save_dir,'bash_script_log.txt')}\"\n",
    "        else:\n",
    "            raise ValueError(f'{task_name} not supported.')\n",
    "\n",
    "        num_gpus = task_config.num_gpus            \n",
    "        cmd = multiline_to_singleline(cmd)\n",
    "        cmds.append(cmd)\n",
    "        \n",
    "        shell_scripts = shell_scripts_template.format(\n",
    "            profile='/fsx/wpq/.profile_local.sh',\n",
    "            conda_env='llava',\n",
    "            cwd=os.getcwd(),\n",
    "            cmd=cmd,\n",
    "            log_dir=log_dir,\n",
    "            save_dir=save_dir\n",
    "        )\n",
    "        out = submit_job_slurm(\n",
    "            shell_scripts, \n",
    "            job_name=job_name,\n",
    "            partition='learnai4p',\n",
    "            nodes=nodes,\n",
    "            num_cpus=num_cpus,\n",
    "            cpu_mem=cpu_mem,\n",
    "            num_gpus=num_gpus,\n",
    "        log_path=os.path.join(log_dir, '%J.out'),\n",
    "            test_run=test_run,\n",
    "        )\n",
    "        for x in out:\n",
    "            x.update({'cmd': cmd})\n",
    "\n",
    "        print(json.dumps(out[0], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff0e0a43-fec6-44eb-85b5-3edf31949182",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.model.builder import load_pretrained_model\n",
    "import os\n",
    "\n",
    "model_path = '/fsx/wpq/github/metasummer2024/external/LLaVA/results/sft_1/lm=Llama-2-7b-hf_vis=clip-vit-large-patch14-336_mm=mlp2x_gelu'\n",
    "model_path = os.path.expanduser(model_path)\n",
    "\n",
    "model_name = get_model_name_from_path(model_path)\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(model_path, args.model_base, model_name)\n",
    "# load_pretrained_model(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5c500d1-5cb2-40f2-a8da-d475bbd381bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 264\n",
      "drwxr-x---   6 wpq  wpq  33280 Jun  7 17:04 .\n",
      "drwxr-xr-t 151 root root 49664 Jun 11 14:18 ..\n",
      "drwxrwxr-x   3 wpq  wpq  33280 Jun  5 18:24 .cache\n",
      "-rw-rw-r--   1 wpq  wpq    727 Jun 12 22:51 .profile_local.sh\n",
      "drwxrwxr-x   2 wpq  wpq  74240 Jun 13 22:20 .sbatch\n",
      "drwxrwxr-x   5 wpq  wpq  33280 Jun  5 03:40 github\n",
      "drwxrwxr-x  19 wpq  wpq  33280 Jun  4 23:34 miniconda3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "960e5572-0f70-4bda-98a5-a1ed9532b912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:llava]",
   "language": "python",
   "name": "conda-env-llava-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
