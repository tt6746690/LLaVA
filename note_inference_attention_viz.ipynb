{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604bef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rosemary import jpt_setup; jpt_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1073e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import process_images, get_model_name_from_path, process_images, tokenizer_image_token\n",
    "from llava.eval.run_llava import eval_model\n",
    "\n",
    "from llava.constants import (\n",
    "    IMAGE_TOKEN_INDEX,\n",
    "    DEFAULT_IMAGE_TOKEN,\n",
    "    DEFAULT_IM_START_TOKEN,\n",
    "    DEFAULT_IM_END_TOKEN,\n",
    "    IMAGE_PLACEHOLDER,\n",
    ")\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.eval.run_llava import image_parser, load_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa150ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"liuhaotian/llava-v1.5-7b\"\n",
    "# prompt = \"What are the things I should be cautious about when I visit here?\"\n",
    "prompt = \"What is unusual about this image?\"\n",
    "image_file = \"https://llava-vl.github.io/static/images/view.jpg\"\n",
    "image_file = 'taxi_iron_man.jpeg'\n",
    "\n",
    "args = type('Args', (), {\n",
    "    \"model_path\": model_path,\n",
    "    \"model_base\": None,\n",
    "    \"model_name\": get_model_name_from_path(model_path),\n",
    "    \"query\": prompt,\n",
    "    \"conv_mode\": None,\n",
    "    \"image_file\": image_file,\n",
    "    \"sep\": \",\",\n",
    "    \"temperature\": 0,\n",
    "    \"top_p\": None,\n",
    "    \"num_beams\": 1,\n",
    "    \"max_new_tokens\": 512\n",
    "})()\n",
    "\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182f8b68",
   "metadata": {},
   "source": [
    "## llava.eval.run_llava.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b529bdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = get_model_name_from_path(args.model_path)\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    args.model_path, args.model_base, model_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c03e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qs = args.query\n",
    "image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\n",
    "\n",
    "print(qs)\n",
    "\n",
    "if IMAGE_PLACEHOLDER in qs:\n",
    "    if model.config.mm_use_im_start_end:\n",
    "        qs = re.sub(IMAGE_PLACEHOLDER, image_token_se, qs)\n",
    "    else:\n",
    "        qs = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, qs)\n",
    "else:\n",
    "    if model.config.mm_use_im_start_end:\n",
    "        qs = image_token_se + \"\\n\" + qs\n",
    "    else:\n",
    "        qs = DEFAULT_IMAGE_TOKEN + \"\\n\" + qs\n",
    "\n",
    "qs, image_token_se, IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1759c36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"llama-2\" in model_name.lower():\n",
    "    conv_mode = \"llava_llama_2\"\n",
    "elif \"mistral\" in model_name.lower():\n",
    "    conv_mode = \"mistral_instruct\"\n",
    "elif \"v1.6-34b\" in model_name.lower():\n",
    "    conv_mode = \"chatml_direct\"\n",
    "elif \"v1\" in model_name.lower():\n",
    "    conv_mode = \"llava_v1\"\n",
    "elif \"mpt\" in model_name.lower():\n",
    "    conv_mode = \"mpt\"\n",
    "else:\n",
    "    conv_mode = \"llava_v0\"\n",
    "\n",
    "\n",
    "if args.conv_mode is not None and conv_mode != args.conv_mode:\n",
    "    print(\n",
    "        \"[WARNING] the auto inferred conversation mode is {}, while `--conv-mode` is {}, using {}\".format(\n",
    "            conv_mode, args.conv_mode, args.conv_mode\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    args.conv_mode = conv_mode\n",
    "    \n",
    "model_name, conv_mode, args.conv_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244cd510",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conv = conv_templates[args.conv_mode].copy()\n",
    "conv.append_message(conv.roles[0], qs)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt = conv.get_prompt()\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8530bb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_files = image_parser(args)\n",
    "images = load_images(image_files)\n",
    "image_sizes = [x.size for x in images]\n",
    "\n",
    "print(image_sizes)\n",
    "images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519f50ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "images_tensor = process_images(\n",
    "    images,\n",
    "    image_processor,\n",
    "    model.config\n",
    ").to(model.device, dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56089b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.imshow(images_tensor[0].cpu().numpy().transpose((1, 2, 0)).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e6d8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenizer_image_token -> substitute <image> with -200.\n",
    "input_ids = (\n",
    "    tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n",
    "    .unsqueeze(0)\n",
    "    .cuda()\n",
    ")\n",
    "\n",
    "l1 = torch.tensor([[    1,   319, 13563,  1546,   263, 12758,  5199,   322,   385, 23116,\n",
    "         21082, 20255, 29889,   450, 20255,  4076,  8444, 29892, 13173, 29892,\n",
    "           322,  1248,   568,  6089,   304,   278,  5199, 29915, 29879,  5155,\n",
    "         29889,  3148,  1001, 29901, 29871, ]])\n",
    "l2 = torch.tensor([[29871,    13,  5618,   526,\n",
    "           278,  2712,   306,   881,   367,   274,  1300,  2738,  1048,   746,\n",
    "           306,  6493,  1244, 29973,   319,  1799,  9047, 13566, 29901]])\n",
    "tokenizer.decode(l1[0]), tokenizer.decode(l2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55684f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.inference_mode():\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        images=images_tensor,\n",
    "        image_sizes=image_sizes,\n",
    "        do_sample=True if args.temperature > 0 else False,\n",
    "        temperature=args.temperature,\n",
    "        top_p=args.top_p,\n",
    "        num_beams=args.num_beams,\n",
    "        max_new_tokens=args.max_new_tokens,\n",
    "        use_cache=True,\n",
    "        return_dict=True,\n",
    "    )\n",
    "\n",
    "\n",
    "outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e2e09b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "input_output_ids = torch.cat([input_ids, output_ids], dim=1)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = model.forward(\n",
    "        input_output_ids,\n",
    "        images=images_tensor,\n",
    "        image_sizes=image_sizes,\n",
    "        use_cache=True,\n",
    "        output_attentions=True,\n",
    "        output_hidden_states=True,\n",
    "        return_dict=True,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15c5249",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = input_output_ids.detach().cpu().clone()[0]\n",
    "\n",
    "im_ind = torch.where(x == IMAGE_TOKEN_INDEX)[0].tolist()[0]\n",
    "tokens = tokenizer.convert_ids_to_tokens(x[:im_ind]) + ['im_'+str(i) for i in range((outputs.attentions[0].shape[-1]-len(x)+1))] + tokenizer.convert_ids_to_tokens(x[im_ind+1:])\n",
    "len(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084e469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "attns = [torch.mean(outputs.attentions[-1][:,:,:n,:n], dim=1, keepdim=True)]\n",
    "toks = tokens[:n]\n",
    "attns[0].shape, len(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cc842b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bertviz import head_view\n",
    "\n",
    "head_view(attns, toks[::-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59dd0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertviz import head_view, model_view\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"distilbert-base-uncased\", output_attentions=True)\n",
    "inputs = tokenizer.encode(\"The cat sat on the mat\", return_tensors='pt')\n",
    "outputs = model(inputs)\n",
    "attention = outputs[-1]  # Output includes attention weights when output_attentions=True\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[0]) \n",
    "model_view(attention, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8f3aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "[x.shape for x in attention]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d727d882",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_view(attention, tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:llava]",
   "language": "python",
   "name": "conda-env-llava-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
