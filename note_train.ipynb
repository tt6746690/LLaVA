{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57ff0c15-4d09-4c5e-8798-e22467194b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/fsx/wpq/github/metasummer2024/external/LLaVA') # jupyter lab moving ipynb does not change !pwd properly.\n",
    "os.environ['WANDB_DIR'] = '/fsx/wpq/github/metasummer2024/cache'\n",
    "os.makedirs(os.environ['WANDB_DIR'], exist_ok=True)\n",
    "os.environ['WANDB_PROJECT'] = 'meta'\n",
    "\n",
    "import re\n",
    "import pathlib\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from llava import conversation as conversation_lib\n",
    "from llava.model import *\n",
    "from llava.train.llava_trainer import LLaVATrainer\n",
    "\n",
    "from llava.train.train import (\n",
    "    ModelArguments, DataArguments, TrainingArguments,\n",
    "    maybe_zero_3, get_peft_state_maybe_zero_3, get_peft_state_non_lora_maybe_zero_3, get_mm_adapter_state_maybe_zero_3,\n",
    "    find_all_linear_names, safe_save_model_for_hf_trainer,\n",
    "    smart_tokenizer_and_embedding_resize,\n",
    "    _tokenize_fn,\n",
    "    _mask_targets,\n",
    "    _add_speaker_and_signal,\n",
    "    preprocess_multimodal,\n",
    "    preprocess,\n",
    "    LazySupervisedDataset,\n",
    "    DataCollatorForSupervisedDataset,\n",
    "    make_supervised_data_module,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0daf1ca7-219a-4c8a-bba7-a64e6a3c1c30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \\\n",
      "\t--deepspeed ./scripts/zero2.json \\\n",
      "\t--model_name_or_path ./results/baselines/lmsys/vicuna-7b-v1.5 \\\n",
      "\t--version plain \\\n",
      "\t--data_path ./data/liuhaotian/LLaVA-Instruct-150K/llava_v1_5_mix665k.json \\\n",
      "\t--image_folder ./data/ \\\n",
      "\t--vision_tower ./results/baselines/openai/clip-vit-large-patch14-336 \\\n",
      "\t--mm_projector_type mlp2x_gelu \\\n",
      "\t--tune_mm_mlp_adapter True \\\n",
      "\t--mm_vision_select_layer -2 \\\n",
      "\t--mm_use_im_start_end False \\\n",
      "\t--mm_use_im_patch_token False \\\n",
      "\t--bf16 True \\\n",
      "\t--output_dir ./results/sft/llava-v1.5-7b \\\n",
      "\t--train_size 96 \\\n",
      "\t--num_train_epochs 1 \\\n",
      "\t--per_device_train_batch_size 32 \\\n",
      "\t--per_device_eval_batch_size 4 \\\n",
      "\t--gradient_accumulation_steps 1 \\\n",
      "\t--evaluation_strategy \"no\" \\\n",
      "\t--save_strategy \"steps\" \\\n",
      "\t--save_steps 24000 \\\n",
      "\t--save_total_limit 1 \\\n",
      "\t--learning_rate 1e-3 \\\n",
      "\t--weight_decay 0. \\\n",
      "\t--warmup_ratio 0.03 \\\n",
      "\t--lr_scheduler_type \"cosine\" \\\n",
      "\t--logging_steps 1 \\\n",
      "\t--tf32 True \\\n",
      "\t--model_max_length 2048 \\\n",
      "\t--gradient_checkpointing True \\\n",
      "\t--dataloader_num_workers 4 \\\n",
      "\t--lazy_preprocess True \\\n",
      "\t--report_to wandb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(ModelArguments(model_name_or_path='./results/baselines/lmsys/vicuna-7b-v1.5', version='plain', freeze_backbone=False, tune_mm_mlp_adapter=True, vision_tower='./results/baselines/openai/clip-vit-large-patch14-336', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch'),\n",
       " DataArguments(data_path='./data/liuhaotian/LLaVA-Instruct-150K/llava_v1_5_mix665k.json', lazy_preprocess=True, is_multimodal=False, image_folder='./data/', image_aspect_ratio='square', train_size=96),\n",
       " TrainingArguments(output_dir='./results/sft/llava-v1.5-7b', overwrite_output_dir=False, do_train=False, do_eval=False, do_predict=False, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=4, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.001, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=<SchedulerType.COSINE: 'cosine'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='./results/sft/llava-v1.5-7b/runs/Jun07_16-27-22_a100-st2-p4de24xlarge-1', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=1.0, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=24000, save_total_limit=1, save_safetensors=True, save_on_each_node=False, save_only_model=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=True, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=4, past_index=-1, run_name='./results/sft/llava-v1.5-7b', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed='./scripts/zero2.json', label_smoothing_factor=0.0, optim=<OptimizerNames.ADAMW_TORCH: 'adamw_torch'>, optim_args=None, adafactor=False, group_by_length=False, length_column_name='length', report_to=['wandb'], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, hub_always_push=False, gradient_checkpointing=True, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, dispatch_batches=None, split_batches=False, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, cache_dir=None, freeze_mm_mlp_adapter=False, mpt_attn_impl='triton', model_max_length=2048, double_quant=True, quant_type='nf4', bits=16, lora_enable=False, lora_r=64, lora_alpha=16, lora_dropout=0.05, lora_weight_path='', lora_bias='none', mm_projector_lr=None, group_by_modality_length=False))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_implementation = 'flash_attention_2'\n",
    "\n",
    "# ## pretrain\n",
    "# model_name_or_path = './results/baselines/lmsys/vicuna-7b-v1.5'\n",
    "# data_path = './data/liuhaotian/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json'\n",
    "# image_folder = './data/liuhaotian/LLaVA-Pretrain/images'\n",
    "# vision_tower = './results/baselines/openai/clip-vit-large-patch14-336'\n",
    "# mm_projector_type = 'mlp2x_gelu'\n",
    "# train_size = 96\n",
    "# output_dir = './results/pretrain/llava-v1.5-7b'\n",
    "\n",
    "\n",
    "## finetune \n",
    "model_name_or_path = './results/baselines/lmsys/vicuna-7b-v1.5'\n",
    "pretrain_mm_mlp_adapter = './results/pretrain/llava-v1.5-7b/mm_projector.bin'\n",
    "data_path = './data/liuhaotian/LLaVA-Instruct-150K/llava_v1_5_mix665k.json'\n",
    "image_folder = './data/'\n",
    "vision_tower = './results/baselines/openai/clip-vit-large-patch14-336'\n",
    "mm_projector_type = 'mlp2x_gelu'\n",
    "train_size = 96\n",
    "output_dir = './results/sft/llava-v1.5-7b'\n",
    "\n",
    "\n",
    "cmd = f\"\"\"\n",
    "    --deepspeed ./scripts/zero2.json \\\n",
    "    --model_name_or_path {model_name_or_path} \\\n",
    "    --version plain \\\n",
    "    --data_path {data_path} \\\n",
    "    --image_folder {image_folder} \\\n",
    "    --vision_tower {vision_tower} \\\n",
    "    --mm_projector_type {mm_projector_type} \\\n",
    "    --tune_mm_mlp_adapter True \\\n",
    "    --mm_vision_select_layer -2 \\\n",
    "    --mm_use_im_start_end False \\\n",
    "    --mm_use_im_patch_token False \\\n",
    "    --bf16 True \\\n",
    "    --output_dir {output_dir} \\\n",
    "    {\"--train_size \" + str(train_size) if train_size else \"\"} \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --per_device_train_batch_size 32 \\\n",
    "    --per_device_eval_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --evaluation_strategy \"no\" \\\n",
    "    --save_strategy \"steps\" \\\n",
    "    --save_steps 24000 \\\n",
    "    --save_total_limit 1 \\\n",
    "    --learning_rate 1e-3 \\\n",
    "    --weight_decay 0. \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --lr_scheduler_type \"cosine\" \\\n",
    "    --logging_steps 1 \\\n",
    "    --tf32 True \\\n",
    "    --model_max_length 2048 \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --dataloader_num_workers 4 \\\n",
    "    --lazy_preprocess True \\\n",
    "    --report_to wandb\n",
    "\"\"\"\n",
    "import shlex\n",
    "args = shlex.split(cmd)\n",
    "\n",
    "\n",
    "parser = transformers.HfArgumentParser(\n",
    "    (ModelArguments, DataArguments, TrainingArguments))\n",
    "model_args, data_args, training_args = parser.parse_args_into_dataclasses(args)\n",
    "local_rank = training_args.local_rank\n",
    "compute_dtype = (torch.float16 if training_args.fp16 else (torch.bfloat16 if training_args.bf16 else torch.float32))\n",
    "\n",
    "model_args, data_args, training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e9ef0a-11df-4063-a1a8-22579a66c14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data_path = './data/liuhaotian/LLaVA-Instruct-150K/llava_v1_5_mix665k.json'\n",
    "\n",
    "\n",
    "image_folder = './data/'\n",
    "\n",
    "list_data_dict = json.load(open(data_path, \"r\"))\n",
    "print(f'#examples: {len(list_data_dict)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c956cb85-fbe9-4268-9bd9-756da5a29e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "example = list_data_dict[0]\n",
    "\n",
    "def file_missing(example):\n",
    "    if 'image' in example:\n",
    "        image_file = example['image']\n",
    "        image_path = os.path.join(image_folder, image_file)\n",
    "        return not os.path.isfile(image_path)\n",
    "    else:\n",
    "        # text-only example, assume file is not missing.\n",
    "        return False\n",
    "\n",
    "list_data_dict_file_missing = list(filter(file_missing, list_data_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba54700-609e-4f05-85c4-4a102f41b427",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bnb_model_from_pretrained_args = {}\n",
    "if training_args.bits in [4, 8]:\n",
    "    from transformers import BitsAndBytesConfig\n",
    "    bnb_model_from_pretrained_args.update(dict(\n",
    "        device_map={\"\": training_args.device},\n",
    "        load_in_4bit=training_args.bits == 4,\n",
    "        load_in_8bit=training_args.bits == 8,\n",
    "        quantization_config=BitsAndBytesConfig(\n",
    "            load_in_4bit=training_args.bits == 4,\n",
    "            load_in_8bit=training_args.bits == 8,\n",
    "            llm_int8_skip_modules=[\"mm_projector\"],\n",
    "            llm_int8_threshold=6.0,\n",
    "            llm_int8_has_fp16_weight=False,\n",
    "            bnb_4bit_compute_dtype=compute_dtype,\n",
    "            bnb_4bit_use_double_quant=training_args.double_quant,\n",
    "            bnb_4bit_quant_type=training_args.quant_type # {'fp4', 'nf4'}\n",
    "        )\n",
    "    ))\n",
    "\n",
    "bnb_model_from_pretrained_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ba3267-77aa-48d0-a4eb-1d855b97f1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if model_args.vision_tower is not None:\n",
    "    if 'mpt' in model_args.model_name_or_path:\n",
    "        config = transformers.AutoConfig.from_pretrained(model_args.model_name_or_path, trust_remote_code=True)\n",
    "        config.attn_config['attn_impl'] = training_args.mpt_attn_impl\n",
    "        model = LlavaMptForCausalLM.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            config=config,\n",
    "            cache_dir=training_args.cache_dir,\n",
    "            **bnb_model_from_pretrained_args\n",
    "        )\n",
    "    else:\n",
    "        model = LlavaLlamaForCausalLM.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            cache_dir=training_args.cache_dir,\n",
    "            attn_implementation=attn_implementation,\n",
    "            torch_dtype=(torch.bfloat16 if training_args.bf16 else None),\n",
    "            **bnb_model_from_pretrained_args\n",
    "        )\n",
    "else:\n",
    "    model = transformers.LlamaForCausalLM.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        cache_dir=training_args.cache_dir,\n",
    "        attn_implementation=attn_implementation,\n",
    "        torch_dtype=(torch.bfloat16 if training_args.bf16 else None),\n",
    "        **bnb_model_from_pretrained_args\n",
    "    )\n",
    "model.config.use_cache = False\n",
    "\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea373ad-178a-42f2-bcf6-a5caa1eb7ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if model_args.freeze_backbone:\n",
    "    model.model.requires_grad_(False)\n",
    "\n",
    "if training_args.bits in [4, 8]:\n",
    "    from peft import prepare_model_for_kbit_training\n",
    "    model.config.torch_dtype=(torch.float32 if training_args.fp16 else (torch.bfloat16 if training_args.bf16 else torch.float32))\n",
    "    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=training_args.gradient_checkpointing)\n",
    "\n",
    "if training_args.gradient_checkpointing:\n",
    "    if hasattr(model, \"enable_input_require_grads\"):\n",
    "        model.enable_input_require_grads()\n",
    "    else:\n",
    "        def make_inputs_require_grad(module, input, output):\n",
    "            output.requires_grad_(True)\n",
    "        model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n",
    "\n",
    "if training_args.lora_enable:\n",
    "    from peft import LoraConfig, get_peft_model\n",
    "    lora_config = LoraConfig(\n",
    "        r=training_args.lora_r,\n",
    "        lora_alpha=training_args.lora_alpha,\n",
    "        target_modules=find_all_linear_names(model),\n",
    "        lora_dropout=training_args.lora_dropout,\n",
    "        bias=training_args.lora_bias,\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    if training_args.bits == 16:\n",
    "        if training_args.bf16:\n",
    "            model.to(torch.bfloat16)\n",
    "        if training_args.fp16:\n",
    "            model.to(torch.float16)\n",
    "    rank0_print(\"Adding LoRA adapters...\")\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b75c2e-310c-4427-8edf-8e5eff2d4bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'mpt' in model_args.model_name_or_path:\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        cache_dir=training_args.cache_dir,\n",
    "        model_max_length=training_args.model_max_length,\n",
    "        padding_side=\"right\"\n",
    "    )\n",
    "else:\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        cache_dir=training_args.cache_dir,\n",
    "        model_max_length=training_args.model_max_length,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=False,\n",
    "    )\n",
    "\n",
    "if model_args.version == \"v0\":\n",
    "    if tokenizer.pad_token is None:\n",
    "        smart_tokenizer_and_embedding_resize(\n",
    "            special_tokens_dict=dict(pad_token=\"[PAD]\"),\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "        )\n",
    "elif model_args.version == \"v0.5\":\n",
    "    tokenizer.pad_token = tokenizer.unk_token\n",
    "else:\n",
    "    tokenizer.pad_token = tokenizer.unk_token\n",
    "    if model_args.version in conversation_lib.conv_templates:\n",
    "        conversation_lib.default_conversation = conversation_lib.conv_templates[model_args.version]\n",
    "    else:\n",
    "        conversation_lib.default_conversation = conversation_lib.conv_templates[\"vicuna_v1\"]\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cf030e-b952-475d-a49a-d94d5e5660a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if model_args.vision_tower is not None:\n",
    "    model.get_model().initialize_vision_modules(\n",
    "        model_args=model_args,\n",
    "        fsdp=training_args.fsdp\n",
    "    )\n",
    "    \n",
    "    vision_tower = model.get_vision_tower()\n",
    "    vision_tower.to(dtype=torch.bfloat16 if training_args.bf16 else torch.float16, device=training_args.device)\n",
    "\n",
    "    data_args.image_processor = vision_tower.image_processor\n",
    "    data_args.is_multimodal = True\n",
    "\n",
    "    model.config.image_aspect_ratio = data_args.image_aspect_ratio\n",
    "    model.config.tokenizer_padding_side = tokenizer.padding_side\n",
    "    model.config.tokenizer_model_max_length = tokenizer.model_max_length\n",
    "\n",
    "    # wpq: not sure why set `tune_mm_mlp_adapter` for `training_args`.\n",
    "    model.config.tune_mm_mlp_adapter = training_args.tune_mm_mlp_adapter = model_args.tune_mm_mlp_adapter\n",
    "\n",
    "    # wpq:\n",
    "    # pretrain: tune_mm_mlp_adapter=True, freeze LLM, optimize adapter only.\n",
    "    # sft: tune_mm_mlp_adapter=False, optimize LLM & adapter jointly.\n",
    "    #\n",
    "    # \n",
    "    \n",
    "    if model_args.tune_mm_mlp_adapter:\n",
    "        model.requires_grad_(False)\n",
    "        for p in model.get_model().mm_projector.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    model.config.freeze_mm_mlp_adapter = training_args.freeze_mm_mlp_adapter\n",
    "    if training_args.freeze_mm_mlp_adapter:\n",
    "        for p in model.get_model().mm_projector.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    if training_args.bits in [4, 8]:\n",
    "        model.get_model().mm_projector.to(dtype=compute_dtype, device=training_args.device)\n",
    "\n",
    "    model.config.mm_use_im_start_end = data_args.mm_use_im_start_end = model_args.mm_use_im_start_end\n",
    "    model.config.mm_projector_lr = training_args.mm_projector_lr\n",
    "    training_args.use_im_start_end = model_args.mm_use_im_start_end\n",
    "    model.config.mm_use_im_patch_token = model_args.mm_use_im_patch_token\n",
    "    model.initialize_vision_tokenizer(model_args, tokenizer=tokenizer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87047ee1-6f2b-47af-bbec-4ecdbe7342ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if training_args.bits in [4, 8]:\n",
    "    from peft.tuners.lora import LoraLayer\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, LoraLayer):\n",
    "            if training_args.bf16:\n",
    "                module = module.to(torch.bfloat16)\n",
    "        if 'norm' in name:\n",
    "            module = module.to(torch.float32)\n",
    "        if 'lm_head' in name or 'embed_tokens' in name:\n",
    "            if hasattr(module, 'weight'):\n",
    "                if training_args.bf16 and module.weight.dtype == torch.float32:\n",
    "                    module = module.to(torch.bfloat16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ed6d8d-9e02-48d7-989b-54de32668d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_module = make_supervised_data_module(tokenizer=tokenizer,\n",
    "                                          data_args=data_args)\n",
    "data_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977be08e-ce5a-490f-8069-366bc7a3dcd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_module['train_dataset'].list_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8491b60-a587-4794-959a-8dec55b3f1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = data_module['train_dataset']\n",
    "data = ds[6]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(data['image'].numpy().transpose((1,2,0)))\n",
    "print(tokenizer.decode(data['input_ids'].tolist()[2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507a8b88-f53c-4ec3-b421-076e14be894a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = LLaVATrainer(model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                args=training_args,\n",
    "                **data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035a7900-3753-436f-956b-ecb299552f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if list(pathlib.Path(training_args.output_dir).glob(\"checkpoint-*\")):\n",
    "    trainer.train(resume_from_checkpoint=True)\n",
    "else:\n",
    "    trainer.train()\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ab659f-82f2-49a0-b816-9e6a24ad11f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.config.use_cache = True\n",
    "\n",
    "if training_args.lora_enable:\n",
    "    state_dict = get_peft_state_maybe_zero_3(\n",
    "        model.named_parameters(), training_args.lora_bias\n",
    "    )\n",
    "    non_lora_state_dict = get_peft_state_non_lora_maybe_zero_3(\n",
    "        model.named_parameters()\n",
    "    )\n",
    "    if training_args.local_rank == 0 or training_args.local_rank == -1:\n",
    "        model.config.save_pretrained(training_args.output_dir)\n",
    "        model.save_pretrained(training_args.output_dir, state_dict=state_dict)\n",
    "        torch.save(non_lora_state_dict, os.path.join(training_args.output_dir, 'non_lora_trainables.bin'))\n",
    "else:\n",
    "    safe_save_model_for_hf_trainer(trainer=trainer,\n",
    "                                   output_dir=training_args.output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:llava]",
   "language": "python",
   "name": "conda-env-llava-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
