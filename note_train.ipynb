{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ff0c15-4d09-4c5e-8798-e22467194b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/fsx/wpq/github/metasummer2024/external/LLaVA') # jupyter lab moving ipynb does not change !pwd properly.\n",
    "os.environ['WANDB_DIR'] = '/fsx/wpq/github/metasummer2024/cache'\n",
    "os.makedirs(os.environ['WANDB_DIR'], exist_ok=True)\n",
    "os.environ['WANDB_PROJECT'] = 'meta'\n",
    "\n",
    "import pathlib\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from llava import conversation as conversation_lib\n",
    "from llava.model import *\n",
    "from llava.train.llava_trainer import LLaVATrainer\n",
    "\n",
    "from llava.train.train import (\n",
    "    ModelArguments, DataArguments, TrainingArguments,\n",
    "    maybe_zero_3, get_peft_state_maybe_zero_3, get_peft_state_non_lora_maybe_zero_3, get_mm_adapter_state_maybe_zero_3,\n",
    "    find_all_linear_names, safe_save_model_for_hf_trainer,\n",
    "    smart_tokenizer_and_embedding_resize,\n",
    "    _tokenize_fn,\n",
    "    _mask_targets,\n",
    "    _add_speaker_and_signal,\n",
    "    preprocess_multimodal,\n",
    "    preprocess,\n",
    "    LazySupervisedDataset,\n",
    "    DataCollatorForSupervisedDataset,\n",
    "    make_supervised_data_module,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daf1ca7-219a-4c8a-bba7-a64e6a3c1c30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attn_implementation = 'flash_attention_2'\n",
    "\n",
    "model_name_or_path = './results/baselines/lmsys/vicuna-7b-v1.5'\n",
    "data_path = './data/liuhaotian/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json'\n",
    "image_folder = './data/liuhaotian/LLaVA-Pretrain/images'\n",
    "vision_tower = './results/baselines/openai/clip-vit-large-patch14-336'\n",
    "mm_projector_type = 'mlp2x_gelu'\n",
    "train_size = 96\n",
    "\n",
    "output_dir = './results/pretrain/llava-v1.5-7b'\n",
    "\n",
    "\n",
    "cmd = f\"\"\"\n",
    "    --deepspeed ./scripts/zero2.json \\\n",
    "    --model_name_or_path {model_name_or_path} \\\n",
    "    --version plain \\\n",
    "    --data_path {data_path} \\\n",
    "    --image_folder {image_folder} \\\n",
    "    --vision_tower {vision_tower} \\\n",
    "    --mm_projector_type {mm_projector_type} \\\n",
    "    --tune_mm_mlp_adapter True \\\n",
    "    --mm_vision_select_layer -2 \\\n",
    "    --mm_use_im_start_end False \\\n",
    "    --mm_use_im_patch_token False \\\n",
    "    --bf16 True \\\n",
    "    --output_dir {output_dir} \\\n",
    "    {\"--train_size \" + str(train_size) if train_size else \"\"} \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --per_device_train_batch_size 32 \\\n",
    "    --per_device_eval_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --evaluation_strategy \"no\" \\\n",
    "    --save_strategy \"steps\" \\\n",
    "    --save_steps 24000 \\\n",
    "    --save_total_limit 1 \\\n",
    "    --learning_rate 1e-3 \\\n",
    "    --weight_decay 0. \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --lr_scheduler_type \"cosine\" \\\n",
    "    --logging_steps 1 \\\n",
    "    --tf32 True \\\n",
    "    --model_max_length 2048 \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --dataloader_num_workers 4 \\\n",
    "    --lazy_preprocess True \\\n",
    "    --report_to wandb\n",
    "\"\"\"\n",
    "import shlex\n",
    "args = shlex.split(cmd)\n",
    "\n",
    "\n",
    "parser = transformers.HfArgumentParser(\n",
    "    (ModelArguments, DataArguments, TrainingArguments))\n",
    "model_args, data_args, training_args = parser.parse_args_into_dataclasses(args)\n",
    "local_rank = training_args.local_rank\n",
    "compute_dtype = (torch.float16 if training_args.fp16 else (torch.bfloat16 if training_args.bf16 else torch.float32))\n",
    "\n",
    "model_args, data_args, training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e9ef0a-11df-4063-a1a8-22579a66c14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data_path = './data/liuhaotian/LLaVA-Instruct-150K/llava_v1_5_mix665k.json'\n",
    "\n",
    "\n",
    "image_folder = './data/'\n",
    "\n",
    "list_data_dict = json.load(open(data_path, \"r\"))\n",
    "print(f'#examples: {len(list_data_dict)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c956cb85-fbe9-4268-9bd9-756da5a29e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "example = list_data_dict[0]\n",
    "\n",
    "def file_missing(example):\n",
    "    if 'image' in example:\n",
    "        image_file = example['image']\n",
    "        image_path = os.path.join(image_folder, image_file)\n",
    "        return not os.path.isfile(image_path)\n",
    "    else:\n",
    "        # text-only example, assume file is not missing.\n",
    "        return False\n",
    "\n",
    "list_data_dict_file_missing = list(filter(file_missing, list_data_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba54700-609e-4f05-85c4-4a102f41b427",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bnb_model_from_pretrained_args = {}\n",
    "if training_args.bits in [4, 8]:\n",
    "    from transformers import BitsAndBytesConfig\n",
    "    bnb_model_from_pretrained_args.update(dict(\n",
    "        device_map={\"\": training_args.device},\n",
    "        load_in_4bit=training_args.bits == 4,\n",
    "        load_in_8bit=training_args.bits == 8,\n",
    "        quantization_config=BitsAndBytesConfig(\n",
    "            load_in_4bit=training_args.bits == 4,\n",
    "            load_in_8bit=training_args.bits == 8,\n",
    "            llm_int8_skip_modules=[\"mm_projector\"],\n",
    "            llm_int8_threshold=6.0,\n",
    "            llm_int8_has_fp16_weight=False,\n",
    "            bnb_4bit_compute_dtype=compute_dtype,\n",
    "            bnb_4bit_use_double_quant=training_args.double_quant,\n",
    "            bnb_4bit_quant_type=training_args.quant_type # {'fp4', 'nf4'}\n",
    "        )\n",
    "    ))\n",
    "\n",
    "bnb_model_from_pretrained_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ba3267-77aa-48d0-a4eb-1d855b97f1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if model_args.vision_tower is not None:\n",
    "    if 'mpt' in model_args.model_name_or_path:\n",
    "        config = transformers.AutoConfig.from_pretrained(model_args.model_name_or_path, trust_remote_code=True)\n",
    "        config.attn_config['attn_impl'] = training_args.mpt_attn_impl\n",
    "        model = LlavaMptForCausalLM.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            config=config,\n",
    "            cache_dir=training_args.cache_dir,\n",
    "            **bnb_model_from_pretrained_args\n",
    "        )\n",
    "    else:\n",
    "        model = LlavaLlamaForCausalLM.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            cache_dir=training_args.cache_dir,\n",
    "            attn_implementation=attn_implementation,\n",
    "            torch_dtype=(torch.bfloat16 if training_args.bf16 else None),\n",
    "            **bnb_model_from_pretrained_args\n",
    "        )\n",
    "else:\n",
    "    model = transformers.LlamaForCausalLM.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        cache_dir=training_args.cache_dir,\n",
    "        attn_implementation=attn_implementation,\n",
    "        torch_dtype=(torch.bfloat16 if training_args.bf16 else None),\n",
    "        **bnb_model_from_pretrained_args\n",
    "    )\n",
    "model.config.use_cache = False\n",
    "\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea373ad-178a-42f2-bcf6-a5caa1eb7ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if model_args.freeze_backbone:\n",
    "    model.model.requires_grad_(False)\n",
    "\n",
    "if training_args.bits in [4, 8]:\n",
    "    from peft import prepare_model_for_kbit_training\n",
    "    model.config.torch_dtype=(torch.float32 if training_args.fp16 else (torch.bfloat16 if training_args.bf16 else torch.float32))\n",
    "    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=training_args.gradient_checkpointing)\n",
    "\n",
    "if training_args.gradient_checkpointing:\n",
    "    if hasattr(model, \"enable_input_require_grads\"):\n",
    "        model.enable_input_require_grads()\n",
    "    else:\n",
    "        def make_inputs_require_grad(module, input, output):\n",
    "            output.requires_grad_(True)\n",
    "        model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n",
    "\n",
    "if training_args.lora_enable:\n",
    "    from peft import LoraConfig, get_peft_model\n",
    "    lora_config = LoraConfig(\n",
    "        r=training_args.lora_r,\n",
    "        lora_alpha=training_args.lora_alpha,\n",
    "        target_modules=find_all_linear_names(model),\n",
    "        lora_dropout=training_args.lora_dropout,\n",
    "        bias=training_args.lora_bias,\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    if training_args.bits == 16:\n",
    "        if training_args.bf16:\n",
    "            model.to(torch.bfloat16)\n",
    "        if training_args.fp16:\n",
    "            model.to(torch.float16)\n",
    "    rank0_print(\"Adding LoRA adapters...\")\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b75c2e-310c-4427-8edf-8e5eff2d4bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'mpt' in model_args.model_name_or_path:\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        cache_dir=training_args.cache_dir,\n",
    "        model_max_length=training_args.model_max_length,\n",
    "        padding_side=\"right\"\n",
    "    )\n",
    "else:\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        cache_dir=training_args.cache_dir,\n",
    "        model_max_length=training_args.model_max_length,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=False,\n",
    "    )\n",
    "\n",
    "if model_args.version == \"v0\":\n",
    "    if tokenizer.pad_token is None:\n",
    "        smart_tokenizer_and_embedding_resize(\n",
    "            special_tokens_dict=dict(pad_token=\"[PAD]\"),\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "        )\n",
    "elif model_args.version == \"v0.5\":\n",
    "    tokenizer.pad_token = tokenizer.unk_token\n",
    "else:\n",
    "    tokenizer.pad_token = tokenizer.unk_token\n",
    "    if model_args.version in conversation_lib.conv_templates:\n",
    "        conversation_lib.default_conversation = conversation_lib.conv_templates[model_args.version]\n",
    "    else:\n",
    "        conversation_lib.default_conversation = conversation_lib.conv_templates[\"vicuna_v1\"]\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cf030e-b952-475d-a49a-d94d5e5660a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if model_args.vision_tower is not None:\n",
    "    model.get_model().initialize_vision_modules(\n",
    "        model_args=model_args,\n",
    "        fsdp=training_args.fsdp\n",
    "    )\n",
    "    \n",
    "    vision_tower = model.get_vision_tower()\n",
    "    vision_tower.to(dtype=torch.bfloat16 if training_args.bf16 else torch.float16, device=training_args.device)\n",
    "\n",
    "    data_args.image_processor = vision_tower.image_processor\n",
    "    data_args.is_multimodal = True\n",
    "\n",
    "    model.config.image_aspect_ratio = data_args.image_aspect_ratio\n",
    "    model.config.tokenizer_padding_side = tokenizer.padding_side\n",
    "    model.config.tokenizer_model_max_length = tokenizer.model_max_length\n",
    "\n",
    "    # wpq: not sure why set `tune_mm_mlp_adapter` for `training_args`.\n",
    "    model.config.tune_mm_mlp_adapter = training_args.tune_mm_mlp_adapter = model_args.tune_mm_mlp_adapter\n",
    "\n",
    "    # wpq:\n",
    "    # pretrain: tune_mm_mlp_adapter=True, freeze LLM, optimize adapter only.\n",
    "    # sft: tune_mm_mlp_adapter=False, optimize LLM & adapter jointly.\n",
    "    #\n",
    "    # \n",
    "    \n",
    "    if model_args.tune_mm_mlp_adapter:\n",
    "        model.requires_grad_(False)\n",
    "        for p in model.get_model().mm_projector.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    model.config.freeze_mm_mlp_adapter = training_args.freeze_mm_mlp_adapter\n",
    "    if training_args.freeze_mm_mlp_adapter:\n",
    "        for p in model.get_model().mm_projector.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    if training_args.bits in [4, 8]:\n",
    "        model.get_model().mm_projector.to(dtype=compute_dtype, device=training_args.device)\n",
    "\n",
    "    model.config.mm_use_im_start_end = data_args.mm_use_im_start_end = model_args.mm_use_im_start_end\n",
    "    model.config.mm_projector_lr = training_args.mm_projector_lr\n",
    "    training_args.use_im_start_end = model_args.mm_use_im_start_end\n",
    "    model.config.mm_use_im_patch_token = model_args.mm_use_im_patch_token\n",
    "    model.initialize_vision_tokenizer(model_args, tokenizer=tokenizer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87047ee1-6f2b-47af-bbec-4ecdbe7342ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if training_args.bits in [4, 8]:\n",
    "    from peft.tuners.lora import LoraLayer\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, LoraLayer):\n",
    "            if training_args.bf16:\n",
    "                module = module.to(torch.bfloat16)\n",
    "        if 'norm' in name:\n",
    "            module = module.to(torch.float32)\n",
    "        if 'lm_head' in name or 'embed_tokens' in name:\n",
    "            if hasattr(module, 'weight'):\n",
    "                if training_args.bf16 and module.weight.dtype == torch.float32:\n",
    "                    module = module.to(torch.bfloat16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ed6d8d-9e02-48d7-989b-54de32668d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_module = make_supervised_data_module(tokenizer=tokenizer,\n",
    "                                          data_args=data_args)\n",
    "data_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977be08e-ce5a-490f-8069-366bc7a3dcd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_module['train_dataset'].list_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8491b60-a587-4794-959a-8dec55b3f1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = data_module['train_dataset']\n",
    "data = ds[6]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(data['image'].numpy().transpose((1,2,0)))\n",
    "print(tokenizer.decode(data['input_ids'].tolist()[2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507a8b88-f53c-4ec3-b421-076e14be894a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = LLaVATrainer(model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                args=training_args,\n",
    "                **data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035a7900-3753-436f-956b-ecb299552f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if list(pathlib.Path(training_args.output_dir).glob(\"checkpoint-*\")):\n",
    "    trainer.train(resume_from_checkpoint=True)\n",
    "else:\n",
    "    trainer.train()\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ab659f-82f2-49a0-b816-9e6a24ad11f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.config.use_cache = True\n",
    "\n",
    "if training_args.lora_enable:\n",
    "    state_dict = get_peft_state_maybe_zero_3(\n",
    "        model.named_parameters(), training_args.lora_bias\n",
    "    )\n",
    "    non_lora_state_dict = get_peft_state_non_lora_maybe_zero_3(\n",
    "        model.named_parameters()\n",
    "    )\n",
    "    if training_args.local_rank == 0 or training_args.local_rank == -1:\n",
    "        model.config.save_pretrained(training_args.output_dir)\n",
    "        model.save_pretrained(training_args.output_dir, state_dict=state_dict)\n",
    "        torch.save(non_lora_state_dict, os.path.join(training_args.output_dir, 'non_lora_trainables.bin'))\n",
    "else:\n",
    "    safe_save_model_for_hf_trainer(trainer=trainer,\n",
    "                                   output_dir=training_args.output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:llava]",
   "language": "python",
   "name": "conda-env-llava-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
